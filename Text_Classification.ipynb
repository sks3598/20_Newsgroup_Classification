{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/wikiabhi/Text-Classification/blob/master/Text_Classification.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "eIMe0jYbvPJn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "88L7XhX9vPJr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f547f41-abe6-4d2f-ce22-0c6695235627"
      },
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "start_time"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1528366468.3325632"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "-nFY6BOGvPJz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c53b5dbe-3663-4e0d-816e-611e3562eafc"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /content/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "2-jkrauAvPJ2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F81f1YrqvPJ6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Our own list of some block words to be avoided   \n",
        "block_words = ['newsgroups', 'xref', 'path', 'from', 'subject', 'sender', 'organisation', 'apr','gmt', 'last','better','never','every','even','two','good','used','first','need','going','must','really','might','well','without','made','give','look','try','far','less','seem','new','make','many','way','since','using','take','help','thanks','send','free','may','see','much','want','find','would','one','like','get','use','also','could','say','us','go','please','said','set','got','sure','come','lot','seems','able','anything','put', '--', '|>', '>>', '93', 'xref', 'cantaloupe.srv.cs.cmu.edu', '20', '16', \"max>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'ax>'\", '21', '19', '10', '17', '24', 'reply-to:', 'thu', 'nntp-posting-host:', 're:','25''18'\"i'd\"'>i''22''fri,''23''>the','references:','xref:','sender:','writes:','1993','organization:']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6YzVah-WvPJ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5b2935a-aaa1-4256-f1f1-f1405f5e6447"
      },
      "cell_type": "code",
      "source": [
        "## Download the dataset\n",
        "import urllib.request\n",
        "urllib.request.urlretrieve (\"https://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/20_newsgroups.tar.gz\", \"a.tar.gz\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('a.tar.gz', <http.client.HTTPMessage at 0x7f361d78eac8>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "iwG_lqfovPKC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Extracting the dataset\n",
        "import tarfile\n",
        "tar = tarfile.open(\"a.tar.gz\")\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pgQpJW_3vPKF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "b06b54db-102a-4a46-9f77-8c36681c4488"
      },
      "cell_type": "code",
      "source": [
        "##Make a list of the folders in the dataset\n",
        "directory = [f for f in os.listdir('./20_newsgroups') if not f.startswith('.')]\n",
        "directory"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['misc.forsale',\n",
              " 'sci.med',\n",
              " 'talk.politics.guns',\n",
              " 'alt.atheism',\n",
              " 'talk.politics.mideast',\n",
              " 'soc.religion.christian',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'rec.autos',\n",
              " 'sci.crypt',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'sci.electronics',\n",
              " 'comp.graphics',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.motorcycles',\n",
              " 'talk.politics.misc',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'sci.space',\n",
              " 'talk.religion.misc',\n",
              " 'rec.sport.hockey',\n",
              " 'comp.windows.x']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "qwAtJby3vPKK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create a dictionary of words with their frequency\n",
        "vocab = {}\n",
        "for i in range(len(directory)):\n",
        "    ##Create a list of files in the given dictionary \n",
        "    files = os.listdir('./20_newsgroups/' + directory[i])\n",
        " \n",
        "    for j in range(len(files)):\n",
        "        ##Path of each file \n",
        "        path = './20_newsgroups/' + directory[i] + '/' + files[j]\n",
        "        \n",
        "        ##open the file and read it\n",
        "        text = open(path, 'r', errors='ignore').read()\n",
        "        \n",
        "        for word in text.split():\n",
        "            if len(word) != 1: \n",
        "                ##Check if word is a non stop word or non block word(we have created) only then proceed\n",
        "                if not word.lower() in stop_words:\n",
        "                    if not word.lower() in block_words:     \n",
        "                        ##If word is already in dictionary then we just increment its frequency by 1\n",
        "                        if vocab.get(word.lower()) != None:\n",
        "                            vocab[word.lower()] += 1\n",
        "\n",
        "                        ##If word is not in dictionary then we put that word in our dictinary by making its frequnecy 1\n",
        "                        else:\n",
        "                            vocab[word.lower()] = 1\n",
        "            \n",
        "# vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cMStmpcWvPKP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import operator\n",
        "sorted_vocab = sorted(vocab.items(), key= operator.itemgetter(1), reverse= True)\n",
        "# sorted_vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tM1hpnxIvPKU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Dictionary containing the most occuring k-words.\n",
        "kvocab={}\n",
        "\n",
        "# Frequency of 1000th most occured word\n",
        "z = sorted_vocab[2000][1]\n",
        "\n",
        "for x in sorted_vocab:\n",
        "    kvocab[x[0]] = x[1]\n",
        "    \n",
        "    if x[1] <= z:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7mIMaUZEvPKX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        },
        "outputId": "7a0c0b5b-3734-4d64-9c52-60b32f24fcac"
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "sorted_vocab[0:100]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('subject:', 20486),\n",
              " ('from:', 20417),\n",
              " ('date:', 20137),\n",
              " ('newsgroups:', 20081),\n",
              " ('message-id:', 20050),\n",
              " ('lines:', 20042),\n",
              " ('path:', 20029),\n",
              " ('article', 12108),\n",
              " ('people', 8415),\n",
              " ('university', 8203),\n",
              " ('know', 7695),\n",
              " ('think', 7205),\n",
              " (\"i'm\", 5823),\n",
              " ('distribution:', 4406),\n",
              " ('time', 4336),\n",
              " ('it.', 4185),\n",
              " ('anyone', 3976),\n",
              " ('world', 3602),\n",
              " ('right', 3326),\n",
              " ('believe', 3309),\n",
              " ('still', 3290),\n",
              " ('something', 3190),\n",
              " ('computer', 3157),\n",
              " ('system', 3137),\n",
              " (\"i've\", 3114),\n",
              " ('15', 2881),\n",
              " ('god', 2881),\n",
              " ('back', 2840),\n",
              " ('news', 2836),\n",
              " (\"can't\", 2836),\n",
              " ('state', 2787),\n",
              " ('work', 2692),\n",
              " ('someone', 2610),\n",
              " ('>in', 2610),\n",
              " ('government', 2534),\n",
              " ('problem', 2528),\n",
              " ('23', 2522),\n",
              " ('another', 2516),\n",
              " ('read', 2516),\n",
              " ('usa', 2496),\n",
              " ('information', 2480),\n",
              " ('>the', 2452),\n",
              " ('number', 2424),\n",
              " (\"that's\", 2382),\n",
              " ('things', 2378),\n",
              " ('part', 2323),\n",
              " ('fri,', 2307),\n",
              " ('point', 2297),\n",
              " ('little', 2294),\n",
              " ('22', 2284),\n",
              " ('windows', 2265),\n",
              " ('>i', 2253),\n",
              " ('tue,', 2241),\n",
              " ('file', 2208),\n",
              " ('data', 2155),\n",
              " ('question', 2126),\n",
              " ('probably', 2112),\n",
              " ('years', 2106),\n",
              " ('different', 2100),\n",
              " ('available', 2095),\n",
              " ('(usenet', 2079),\n",
              " ('space', 2079),\n",
              " ('it,', 2073),\n",
              " ('around', 2072),\n",
              " ('long', 2053),\n",
              " ('tell', 2048),\n",
              " ('least', 2006),\n",
              " ('best', 1997),\n",
              " ('program', 1995),\n",
              " ('software', 1976),\n",
              " ('public', 1961),\n",
              " ('power', 1958),\n",
              " ('thu,', 1883),\n",
              " ('thing', 1875),\n",
              " ('drive', 1870),\n",
              " ('run', 1869),\n",
              " ('support', 1864),\n",
              " ('however,', 1826),\n",
              " (\"i'd\", 1825),\n",
              " ('18', 1804),\n",
              " ('rather', 1801),\n",
              " ('enough', 1792),\n",
              " ('case', 1791),\n",
              " ('hard', 1786),\n",
              " ('keep', 1770),\n",
              " ('fact', 1767),\n",
              " ('25', 1758),\n",
              " ('let', 1757),\n",
              " ('science', 1753),\n",
              " ('called', 1751),\n",
              " ('great', 1742),\n",
              " ('...', 1738),\n",
              " ('call', 1725),\n",
              " ('looking', 1709),\n",
              " ('mon,', 1690),\n",
              " ('found', 1683),\n",
              " ('real', 1676),\n",
              " ('nothing', 1671),\n",
              " ('26', 1661),\n",
              " ('quite', 1634)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "9MSn-kD4vPKe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "features_list = list(kvocab.keys())\n",
        "\n",
        "## Create a Dataframe containing features_list as columns \n",
        "df = pd.DataFrame(columns = features_list)\n",
        "\n",
        "\n",
        "## Filling the x_train values in dataframe \n",
        "\n",
        "for i in range(len(directory)):\n",
        "    ##Create a list of files in the given dictionary \n",
        "    files = os.listdir('./20_newsgroups/' + directory[i])\n",
        " \n",
        "    for j in range(len(files)):\n",
        "        ##Insert a row at the end of Dataframe with all zeros\n",
        "        df.loc[len(df)] = np.zeros(len(features_list))\n",
        "        \n",
        "        ##Path of each file \n",
        "        path = './20_newsgroups/' + directory[i] + '/' + files[j]\n",
        "        \n",
        "        ##open the file and read it\n",
        "        text = open(path, 'r', errors='ignore').read()\n",
        "        \n",
        "        \n",
        "        for word in text.split():\n",
        "            if word.lower() in features_list:\n",
        "                df[word.lower()][len(df)-1] += 1\n",
        "                \n",
        "\n",
        "# df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yYp-oBlMvPKh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Making the 2d array of x\n",
        "x = df.values\n",
        "\n",
        "## Feature list\n",
        "f_list = list(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F7aZM5Y5vPKm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0bb772d-8224-4730-a87d-c181946f0916"
      },
      "cell_type": "code",
      "source": [
        "## Creating  y array containing labels for classification \n",
        "\n",
        "y = []\n",
        "\n",
        "for i in range(len(directory)):\n",
        "    ##Create a list of files in the given dictionary \n",
        "    files = os.listdir('./20_newsgroups/' + directory[i])\n",
        " \n",
        "    for j in range(len(files)):\n",
        "        y.append(i)\n",
        "\n",
        "y = np.array(y)\n",
        "y.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19997,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "Su3gZaiAvPKr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Splitting the whole dataset for training and testing\n",
        "from sklearn import model_selection\n",
        "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size = 0.25, random_state = 0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HfvyalwsvPKt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implement Multinomial Naive Bayes from sklearn"
      ]
    },
    {
      "metadata": {
        "id": "f575GAmlvPKv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a2c2a80e-b963-4b35-e99e-42a369e7b367"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB()\n",
        "clf.fit(x_train, y_train)\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "train_score = clf.score(x_train, y_train)\n",
        "test_score = clf.score(x_test, y_test)\n",
        "\n",
        "train_score, test_score"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8797759551910382, 0.834)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "H5DGHNU-vPK3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a90dc80-adf4-43d1-8281-05d85111d6c2"
      },
      "cell_type": "code",
      "source": [
        "## Calculating time on our local machine \n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "total_time"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1700.6345119476318"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "4wWwecGDvPK8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Implementing Multinomial Naive Bayes from scratch"
      ]
    },
    {
      "metadata": {
        "id": "oa--ySR2vPK9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fit(x_train, y_train):\n",
        "    \n",
        "    ## dictionary containing the count of words\n",
        "    count = {}\n",
        "       \n",
        "    ## set of all classes \n",
        "    set_class = set(y_train)\n",
        "            \n",
        "    for current_class in set_class:\n",
        "        count[current_class] = {}\n",
        "        count[\"total_data\"] = len(y_train)\n",
        "        \n",
        "        ##Rows whose class is current_class\n",
        "        current_class_rows = (y_train == current_class)\n",
        "        \n",
        "        x_train_current = x_train[current_class_rows]\n",
        "        y_train_current = y_train[current_class_rows]\n",
        "        \n",
        "        sums = 0\n",
        "        for i in range(len(f_list)):\n",
        "            ## For each class, calculating total frequency of a feature \n",
        "            count[current_class][f_list[i]] = x_train_current[:,i].sum()\n",
        "            sums = sums + count[current_class][f_list[i]]\n",
        "        \n",
        "        ##Calculating total count of words of a class\n",
        "        count[current_class][\"total_count\"] = sums\n",
        "        \n",
        "    return count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WbDHsAHPvPK_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def probability(dictionary, row, current_class):\n",
        "    ## class_prob = log of probability of the current class = log(no of documents having class as current_class)/ (total number of documents)\n",
        "    class_prob = np.log(dictionary[current_class][\"total_count\"]) - np.log(dictionary[\"total_data\"])\n",
        "    total_prob = class_prob\n",
        "    \n",
        "    \n",
        "    for i in range(len(row)):\n",
        "        ##Numerator\n",
        "        word_count = dictionary[current_class][f_list[i]] + 1     \n",
        "        ## Denominator\n",
        "        total_count = dictionary[current_class][\"total_count\"] + len(f_list)\n",
        "        ## Add 1 to numerator and len(row) in denominator for laplace correction\n",
        "        \n",
        "        ## Log Probabilty of a word \n",
        "        word_prob = np.log(word_count) - np.log(total_count)\n",
        "        \n",
        "        ##Calculating probability frequency number of times\n",
        "        for j in range(int(row[i])):\n",
        "            total_prob += word_prob\n",
        "        \n",
        "    return total_prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hEf1unI5vPLR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predictSinglePoint(row, dictionary):\n",
        "    classes = dictionary.keys()\n",
        "    \n",
        "    ##Initialising best_prob and best_class as very low count\n",
        "    \n",
        "    best_prob = -1000\n",
        "    best_class = -1\n",
        "    first_iter = True\n",
        "    \n",
        "    for current_class in classes:\n",
        "        if(current_class == \"total_data\"):\n",
        "            continue\n",
        "        \n",
        "        ##Calculating probabilty that the given row belong to current_class\n",
        "        prob_current_class = probability(dictionary, row, current_class)\n",
        "        \n",
        "        ##For first iteration we set the best_prob to be the probabilty that row is of first class and best_class to be first class\n",
        "        ##For rest iteration, we check if the probabilty that row is of the current_class is greater than the best_prob then we update best_prob and best_class.\n",
        "        if(first_iter or prob_current_class > best_prob):\n",
        "            best_prob = prob_current_class\n",
        "            best_class = current_class\n",
        "        \n",
        "        first_iter = False\n",
        "    \n",
        "    ## Return the best class which has maximum probabilty.\n",
        "    return best_class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tQFk8XgOvPLW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(x_test, dictionary):\n",
        "    ## Initialise a list which contain the predictions\n",
        "    y_pred_self = []\n",
        "    \n",
        "    ##Iterate through each row in x_test\n",
        "    for j in range(len(x_test)):\n",
        "        \n",
        "        ##Calculate the prediction of the class to which the row belong to.\n",
        "        pred_class = predictSinglePoint(x_test[j,:], dictionary) \n",
        "        \n",
        "        ##Append the predicted class to our list\n",
        "        y_pred_self.append(pred_class)\n",
        "    \n",
        "    ##Return the list of predictions\n",
        "    return y_pred_self"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-fIdw-GgvPLZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Training the model \n",
        "dictionary = fit(x_train, y_train)\n",
        "\n",
        "##Testing the model \n",
        "y_pred_self = predict(x_test, dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eM3mDerGvPLc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Accuracy Comparison between Sklearn MultinomialNB() and self implementation"
      ]
    },
    {
      "metadata": {
        "id": "PcLVTdyGvPLd",
        "colab_type": "code",
        "colab": {},
        "outputId": "7d197b33-c859-458f-dfca-8062881ec059"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(\"Accuracy for sklearn MultinomialNB() - \", test_score)\n",
        "print(\"Accuracy for self-implemented Naive Bayes - \", accuracy_score(y_test, y_pred_self))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for sklearn MultinomialNB() -  0.8394\n",
            "Accuracy for self implemented Naive Bayes -  0.8422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lFaHBK7UvPLg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classification Report Comparison between Sklearn MultinomialNB() and self implementation"
      ]
    },
    {
      "metadata": {
        "id": "2-HbBV3evPLh",
        "colab_type": "code",
        "colab": {},
        "outputId": "dd3e23af-6f4e-4e73-e515-e2a8901457c5"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(\"Classification report for sklearn MultinomialNB()\",classification_report(y_test, y_pred))\n",
        "print(\"Classification report for self-implemented Naive Bayes \",classification_report(y_test, y_pred_self))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification report for sklearn MultinomialNB()              precision    recall  f1-score   support\n",
            "\n",
            "          0       0.95      0.78      0.85       233\n",
            "          1       0.83      0.90      0.86       253\n",
            "          2       0.87      0.90      0.88       249\n",
            "          3       0.69      0.77      0.73       240\n",
            "          4       0.91      0.94      0.93       236\n",
            "          5       0.80      0.88      0.84       240\n",
            "          6       0.94      0.95      0.95       261\n",
            "          7       0.96      0.89      0.93       269\n",
            "          8       0.95      0.87      0.90       284\n",
            "          9       0.72      0.61      0.66       248\n",
            "         10       0.82      0.94      0.87       231\n",
            "         11       0.93      0.82      0.87       233\n",
            "         12       0.79      0.76      0.78       244\n",
            "         13       0.89      0.82      0.86       256\n",
            "         14       0.80      0.88      0.84       246\n",
            "         15       0.70      0.86      0.77       253\n",
            "         16       0.89      0.87      0.88       248\n",
            "         17       0.96      1.00      0.98       281\n",
            "         18       0.75      0.87      0.81       259\n",
            "         19       0.62      0.42      0.50       236\n",
            "\n",
            "avg / total       0.84      0.84      0.84      5000\n",
            "\n",
            "Classification report for self implemented Naive Bayes               precision    recall  f1-score   support\n",
            "\n",
            "          0       0.95      0.79      0.86       233\n",
            "          1       0.85      0.90      0.87       253\n",
            "          2       0.87      0.90      0.89       249\n",
            "          3       0.69      0.78      0.73       240\n",
            "          4       0.93      0.94      0.93       236\n",
            "          5       0.80      0.87      0.83       240\n",
            "          6       0.94      0.95      0.94       261\n",
            "          7       0.96      0.90      0.93       269\n",
            "          8       0.94      0.87      0.91       284\n",
            "          9       0.72      0.62      0.67       248\n",
            "         10       0.83      0.94      0.88       231\n",
            "         11       0.92      0.84      0.88       233\n",
            "         12       0.79      0.77      0.78       244\n",
            "         13       0.89      0.83      0.86       256\n",
            "         14       0.81      0.88      0.84       246\n",
            "         15       0.70      0.86      0.77       253\n",
            "         16       0.89      0.87      0.88       248\n",
            "         17       0.96      1.00      0.98       281\n",
            "         18       0.77      0.86      0.81       259\n",
            "         19       0.62      0.43      0.51       236\n",
            "\n",
            "avg / total       0.84      0.84      0.84      5000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}